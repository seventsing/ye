{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s):\n",
    "###### Python version:\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the naive nayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "import csv\n",
    "import numpy as np\n",
    "def preprocess(data_path, split_ratio=0.2, seed=None):\n",
    "    '''\n",
    "    prpross the data from data_path --- a csv format csv file\n",
    "    @data_path: the path of the csv format file\n",
    "    @split_ratio: the ratio of the val data \n",
    "    '''\n",
    "    #read data from csv\n",
    "    with open(data_path, newline='') as csvfile:\n",
    "        lines = csv.reader(csvfile, delimiter=',')  \n",
    "        matrix = np.array(list(lines))\n",
    "    print(\"The shape of the inputdata: \",matrix.shape)\n",
    "    classes = matrix[:,-1]\n",
    "    print(\"The calsses of the data: \",set(classes))\n",
    "    \n",
    "    #shuffle the data by using the random seed or if seed == None random else use the seed th shuffle\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    np.random.shuffle(matrix)\n",
    "    \n",
    "    #split the data into train and validation\n",
    "    train, val = matrix[:int(matrix.shape[0]*(1-split_ratio)),:], matrix[int(matrix.shape[0]*(1-split_ratio)):,:]\n",
    "    print(\"train : val = \",str(len(train)) + \" : \" + str(len(val)))\n",
    "    \n",
    "    #split the attribues and the classes\n",
    "    train_X, train_y = train[:,:-1], train[:,-1]\n",
    "    val_X, val_y = val[:,:-1], val[:,-1]\n",
    "    return train_X, train_y, val_X, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\\\n",
    "import collections\n",
    "def train(train_X, train_y):\n",
    "    '''\n",
    "    to train a classicifer by using the train data\n",
    "    @train_X: train data\n",
    "    @train_y: the classes of the train\n",
    "    '''\n",
    "    assert train_X.shape[0] == train_y.shape[0]\n",
    "    attrs_dict = collections.defaultdict(list)\n",
    "    cls_dict = collections.defaultdict(int)\n",
    "    \n",
    "    #get the class prob matrix\n",
    "    for attr, cls in zip(train_X, train_y):\n",
    "        attrs_dict[cls].append(attr)\n",
    "        cls_dict[cls] += 1\n",
    "    cls_preds = {key:len(val)/train_y.shape[0] for key, val in attrs_dict.items()}\n",
    "    \n",
    "    #ingore the missing value, Conditional probability template for a class\n",
    "    attributs_list = [{key:0 for key in set(train_X[:,i]) if key != \"?\"} for i in range(train_X.shape[1])]\n",
    "    \n",
    "    #get the condition prob matrix\n",
    "    cond_dict = {}\n",
    "    for cls, attrs in attrs_dict.items():\n",
    "        attrs = np.array(attrs)\n",
    "        cls_cond = [{key:0 for key in set(train_X[:,i]) if key != \"?\"} for i in range(train_X.shape[1])]\n",
    "        cond_length = len(attrs)\n",
    "        for i in range(train_X.shape[1]):\n",
    "            col =attrs[:,i]\n",
    "            for key in cls_cond[i].keys():\n",
    "                if key == \"?\":\n",
    "                    continue\n",
    "                else:\n",
    "                    cls_cond[i][key] = (list(col[col != \"?\"]).count(key) + 0) / (cond_length + 0)# 1  and len(set(col)) and appliment add_d smooth\n",
    "        cond_dict[cls] = cls_cond\n",
    "    \n",
    "    return cls_preds, cond_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(x, cls_preds, cond_dict):\n",
    "    '''\n",
    "    predict the class for an instance\n",
    "    @x : an instance\n",
    "    @cls_preds, @cond_dict : a trained model \n",
    "    '''\n",
    "    classes = list(cls_preds.keys())\n",
    "    preds = []\n",
    "    for cls, cls_cond in cond_dict.items():\n",
    "        p1 = cls_preds[cls]\n",
    "        p2 = 1\n",
    "        for i in range(len(cls_cond)):\n",
    "            try:\n",
    "                p2 = p2 * cls_cond[i][x[i]]\n",
    "            except KeyError:\n",
    "                p2 *= 1\n",
    "        preds.append(p1 * p2) \n",
    "    return classes[preds.index(max(preds))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(val_X, val_y, cls_preds, cond_dict):\n",
    "    assert len(val_X) == len(val_y)\n",
    "    count = 0\n",
    "    for x, y in zip(val_X, val_y):\n",
    "        try:\n",
    "            if predict(x, cls_preds, cond_dict) == y:\n",
    "                count += 1\n",
    "        except:\n",
    "            print(x)\n",
    "    return round(count / len(val_X), 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def entropy(probabilitys): \n",
    "    '''Computing information entropy using a probabilitys list'''\n",
    "    return sum(-1 *  np.array(probabilitys) * np.log2(probabilitys))\n",
    "\n",
    "def cond_entropy(attr, train_X, train_y):\n",
    "    '''Computing conditional entropy using a dataset'''\n",
    "    target_set = list(set(train_y))\n",
    "    attr_dict = collections.defaultdict(int)\n",
    "    for attr_cls in train_X[:,attr]:\n",
    "        attr_dict[attr_cls] += 1 \n",
    "    cls_prob_dict = {key:value/sum(attr_dict.values())   for key, value in attr_dict.items()}\n",
    "    assert abs(sum(cls_prob_dict.values()) - 1) < 0.00001 , sum(v)\n",
    "    \n",
    "    dict = collections.defaultdict(list)\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        dict[x[attr]].append(y)\n",
    "    count_dict = {key:[value.count(k) for k in target_set] for key, value in dict.items()}\n",
    "    prob_dict = {k:np.array(v)/sum(v) for k, v in count_dict.items()}\n",
    "    \n",
    "    entropy_cls = [entropy([prob for prob in prob_list if prob != 0])  for key, prob_list in prob_dict.items()]\n",
    "    return sum(np.array(list(cls_prob_dict.values())) * np.array(entropy_cls))           \n",
    "\n",
    "def info_gain(attributes, train_X, train_y):\n",
    "    '''calculate the Information Gain of  a set of attributes'''\n",
    "    cls_dict = collections.defaultdict(int)\n",
    "    for attr, cls in zip(train_X, train_y):\n",
    "        cls_dict[cls] += 1\n",
    "    cls_preds = {k:v/len(train_y) for k, v in cls_dict.items()}\n",
    "    \n",
    "    #get information entropy\n",
    "    H_cls = entropy(list(cls_preds.values()))\n",
    "    \n",
    "    #Computing conditional entropy\n",
    "    H_cond = [cond_entropy(attr, train_X, train_y) for attr in attributes]\n",
    "    return H_cls, np.around(H_cls - np.array(H_cond), 6)\n",
    "\n",
    "#info_gain([0,1], cls_preds, cond_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test  on the 9 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================     anneal        ====================================\n",
      "The shape of the inputdata:  (898, 36)\n",
      "The calsses of the data:  {'2', '1', 'U', '3', '5'}\n",
      "train : val =  718 : 180\n",
      "The evaluation ratio on train set is :  0.8983\n",
      "The evaluation ratio on val set is :  0.8889\n",
      "class Entropy, sum(all attrs info_gain) = 1.1898338562043977 , 3.087583999999999\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34] :\n",
      "[0.40909  0.       0.306052 0.051344 0.291082 0.147119 0.213723 0.292235\n",
      " 0.126166 0.141074 0.032488 0.435178 0.038702 0.000438 0.039356 0.021775\n",
      " 0.037997 0.036703 0.       0.117225 0.029754 0.027042 0.       0.015605\n",
      " 0.137181 0.       0.022397 0.018242 0.       0.       0.       0.04324\n",
      " 0.033038 0.019379 0.003959]\n",
      "=======================     breast-cancer        ====================================\n",
      "The shape of the inputdata:  (286, 10)\n",
      "The calsses of the data:  {'recurrence-events', 'no-recurrence-events'}\n",
      "train : val =  228 : 58\n",
      "The evaluation ratio on train set is :  0.7719\n",
      "The evaluation ratio on val set is :  0.7069\n",
      "class Entropy, sum(all attrs info_gain) = 0.8778446951746506 , 0.31258199999999997\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8] :\n",
      "[0.010606 0.002002 0.057171 0.068995 0.053423 0.07701  0.002489 0.015067\n",
      " 0.025819]\n",
      "=======================     car        ====================================\n",
      "The shape of the inputdata:  (1728, 7)\n",
      "The calsses of the data:  {'unacc', 'acc', 'vgood', 'good'}\n",
      "train : val =  1382 : 346\n",
      "The evaluation ratio on train set is :  0.8777\n",
      "The evaluation ratio on val set is :  0.8699\n",
      "class Entropy, sum(all attrs info_gain) = 1.205740970012175 , 0.6864939999999999\n",
      "Info_gain of [0, 1, 2, 3, 4, 5] :\n",
      "[0.096449 0.073704 0.004486 0.219663 0.030008 0.262184]\n",
      "=======================     cmc        ====================================\n",
      "The shape of the inputdata:  (1473, 9)\n",
      "The calsses of the data:  {'Long-term', 'No-use', 'Short-term'}\n",
      "train : val =  1178 : 295\n",
      "The evaluation ratio on train set is :  0.5102\n",
      "The evaluation ratio on val set is :  0.4746\n",
      "class Entropy, sum(all attrs info_gain) = 1.5390345832497476 , 0.30395900000000003\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7] :\n",
      "[0.070906 0.040139 0.10174  0.009821 0.002582 0.030474 0.032511 0.015786]\n",
      "=======================     hepatitis        ====================================\n",
      "The shape of the inputdata:  (155, 14)\n",
      "The calsses of the data:  {'LIVE', 'DIE'}\n",
      "train : val =  124 : 31\n",
      "The evaluation ratio on train set is :  0.8226\n",
      "The evaluation ratio on val set is :  0.871\n",
      "class Entropy, sum(all attrs info_gain) = 0.7346451526501956 , 0.727295\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] :\n",
      "[0.036607 0.015265 0.014491 0.086451 0.083228 0.013806 0.025836 0.019947\n",
      " 0.035451 0.106099 0.128347 0.076834 0.084933]\n",
      "=======================     hypothyroid        ====================================\n",
      "The shape of the inputdata:  (3163, 19)\n",
      "The calsses of the data:  {'negative', 'hypothyroid'}\n",
      "train : val =  2530 : 633\n",
      "The evaluation ratio on train set is :  0.953\n",
      "The evaluation ratio on val set is :  0.9479\n",
      "class Entropy, sum(all attrs info_gain) = 0.27671573102958635 , 0.040916999999999995\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] :\n",
      "[4.480e-04 9.140e-04 1.238e-03 1.480e-04 9.990e-04 1.368e-03 5.420e-04\n",
      " 4.350e-04 4.890e-04 8.980e-04 4.500e-05 7.900e-05 9.354e-03 4.075e-03\n",
      " 5.793e-03 5.768e-03 5.744e-03 2.580e-03]\n",
      "=======================     mushroom        ====================================\n",
      "The shape of the inputdata:  (8124, 23)\n",
      "The calsses of the data:  {'e', 'p'}\n",
      "train : val =  6499 : 1625\n",
      "The evaluation ratio on train set is :  0.9566\n",
      "The evaluation ratio on val set is :  0.9625\n",
      "class Entropy, sum(all attrs info_gain) = 0.9990678968724603 , 4.388075000000001\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] :\n",
      "[0.048797 0.02859  0.036049 0.192379 0.906075 0.014165 0.100883 0.230154\n",
      " 0.416978 0.007517 0.134818 0.284726 0.271894 0.253845 0.241416 0.\n",
      " 0.023817 0.038453 0.318022 0.480705 0.201958 0.156834]\n",
      "=======================     nursery        ====================================\n",
      "The shape of the inputdata:  (12960, 9)\n",
      "The calsses of the data:  {'very_recom', 'spec_prior', 'not_recom', 'recommend', 'priority'}\n",
      "train : val =  10368 : 2592\n",
      "The evaluation ratio on train set is :  0.9045\n",
      "The evaluation ratio on val set is :  0.9055\n",
      "class Entropy, sum(all attrs info_gain) = 1.7164959001837932 , 1.291786\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7] :\n",
      "[0.072935 0.196449 0.005573 0.011886 0.019602 0.004333 0.022233 0.958775]\n",
      "=======================     primary-tumor        ====================================\n",
      "The shape of the inputdata:  (339, 18)\n",
      "The calsses of the data:  {'B', 'T', 'P', 'R', 'F', 'K', 'U', 'A', 'H', 'G', 'N', 'Q', 'M', 'O', 'J', 'C', 'L', 'S', 'V', 'D', 'E'}\n",
      "train : val =  271 : 68\n",
      "The evaluation ratio on train set is :  0.5978\n",
      "The evaluation ratio on val set is :  0.5147\n",
      "class Entropy, sum(all attrs info_gain) = 3.6437400563509663 , 3.3898029999999997\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] :\n",
      "[0.154742 0.33536  0.552353 0.379104 0.212462 0.020367 0.100881 0.067873\n",
      " 0.220522 0.199761 0.067145 0.060254 0.29153  0.127154 0.245889 0.184258\n",
      " 0.170148]\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\"anneal\", \"breast-cancer\",\"car\",\"cmc\",\"hepatitis\",\"hypothyroid\",\"mushroom\",\"nursery\",\"primary-tumor\"]\n",
    "root = \"2019S1-proj1-data/2019S1-proj1-data\"\n",
    "for file_name in csv_files:\n",
    "    print(\"=======================     {}        ====================================\".format(file_name))\n",
    "    #train_X, train_y, val_X, val_y = preprocess(root + \"/{}.csv\".format(file_name))\n",
    "    train_X, train_y, val_X, val_y = preprocess(root + \"/{}.csv\".format(file_name), seed=24)\n",
    "    cls_preds, cond_dict = train(train_X, train_y)\n",
    "    train_eval_ratio = evaluate(train_X, train_y, cls_preds, cond_dict)\n",
    "    print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "    eval_ratio = evaluate(val_X, val_y, cls_preds, cond_dict)\n",
    "    print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "    \n",
    "    X, Y = np.vstack((train_X, val_X)), np.hstack((train_y,val_y))\n",
    "    attrs = [i for i in range(train_X.shape[1])]\n",
    "    H_cls, H_attrs = info_gain(attrs, X, Y)\n",
    "    print(\"class Entropy, sum(all attrs info_gain) = {} , {}\".format(H_cls, sum(H_attrs)))\n",
    "    print(\"Info_gain of {} :\\n{}\".format(attrs,  H_attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Responses For Question1\n",
    "   When calculating the Information Gain of a set of attributes, I find there are many 0 value or some values that are very close to 0, but some are relatively big.I begin to think what the Information Gain really mean. So I start to look up for some the information on the Internet and designed some experiments to explore the specific meaning of these 0 and how can it relative to classes.   \n",
    "   There are the several experiments.I choose three dataset to do this experiment:mushroom.csv,anneal.csv and nursery.csv.Because the number of the items of the mushroom and nursery are very lage, which means reflecting the real situation better.And there are many 0-value info-gain\n",
    "attributes in anneal.I remove the Minimum and maximum value info-gain col in the dataset,And then train again,to see the preformence in both train-set and validation-set.Here are the result:    \n",
    "#####  experiments result\n",
    "| dataset       |  count   | Original(train)   | Rm min(train) | Rm Max(train) |  Original(val)  | Rm min(val) | Rm Max (val)|\n",
    "| ---------------- | :--------: |:--------: | ----: |   ----: | :--------: | ----: |   ----: |\n",
    "| mushroom     | 8124 |0.9548 |    0.9548 |  0.8989 |  0.9495 |   0.9495 |  0.8942 |\n",
    "| anneal | 898| 0.8955 |   0.8928 |  0.8426 | 0.8167 |   0.8167 |  0.8111 |\n",
    "| nursery     | 12960  | 0.9043  |    0.902 |   0.5422 | 0.8989  |   0.892 |   0.5278 |\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "- When we remove the 0-value-info-gain attributes, the prefromenece will change a little or the same with the oridinary version.especificily on the train data. So the 0 value info-gain means that this attribute has mo relation with the calsss\n",
    "- On the nursery dataset,the info-gain list is [0.070746,  0.197377,  0.005211,  0.01221,   0.019903,  0.004543,  0.021412,  0.95699 ], you can find the 8th(index=7) attribute's info-gain is much bigger others'.When remove this col, The correct rate drop from 0.9043 to 0.5422 on train set and from 0.8989\tto 0.5278.  \n",
    "- Therefore, the proportion of information gain in the information gain list reflects the relationship between the attribute and the category.\n",
    "------------------------------------------------------------------------------\n",
    "Below are three experiments,inculding code.\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mushroom dataset\n",
    "the info-gain of 16th(index=15) attributes is 0, the info-gain of 16th(index=15) attributes is the max   \n",
    "remove the two cols to see the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the inputdata:  (8124, 23)\n",
      "The calsses of the data:  {'e', 'p'}\n",
      "train : val =  6499 : 1625\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] :\n",
      "[0.049568 0.029968 0.039343 0.195216 0.907754 0.014447 0.099232 0.231177\n",
      " 0.417177 0.007587 0.134764 0.286848 0.271637 0.253215 0.241543 0.\n",
      " 0.024343 0.038278 0.324224 0.486742 0.202735 0.153975]\n",
      "\n",
      "--------------------      Original           -----------------------------\n",
      "The evaluation ratio on train set is :  0.9548\n",
      "The evaluation ratio on val set is :  0.9495\n",
      "----------      remove the min info-Gain attributes  (0 or almost 0)  ----------------------\n",
      "The evaluation ratio on train set is :  0.9548\n",
      "The evaluation ratio on val set is :  0.9495\n",
      "----------     remove the max info-Gain attributes    -------------------------\n",
      "The evaluation ratio on train set is :  0.8989\n",
      "The evaluation ratio on val set is :  0.8942\n"
     ]
    }
   ],
   "source": [
    "file_name = \"mushroom\"\n",
    "root = \"2019S1-proj1-data/2019S1-proj1-data\"\n",
    "train_X, train_y, val_X, val_y = preprocess(root + \"/{}.csv\".format(file_name), seed=78)\n",
    "attrs = [i for i in range(train_X.shape[1])]\n",
    "H_cls, H_attrs = info_gain(attrs, train_X, train_y)\n",
    "print(\"Info_gain of {} :\\n{}\".format(attrs,  H_attrs))\n",
    "print()\n",
    "print(\"--------------------      Original           -----------------------------\")\n",
    "cls_preds, cond_dict = train(train_X, train_y)\n",
    "train_eval_ratio = evaluate(train_X, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "\n",
    "print(\"----------      remove the min info-Gain attributes  (0 or almost 0)  ----------------------\")\n",
    "train_X_min = np.delete(train_X, 15, axis=1)\n",
    "val_X_min = np.delete(val_X, 15, axis=1)\n",
    "cls_preds, cond_dict = train(train_X_min, train_y)\n",
    "train_eval_ratio = evaluate(train_X_min, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X_min, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "\n",
    "print(\"----------     remove the max info-Gain attributes    -------------------------\")\n",
    "train_X_max = np.delete(train_X, 4, axis=1)\n",
    "val_X_max = np.delete(val_X, 4, axis=1)\n",
    "cls_preds, cond_dict = train(train_X_max, train_y)\n",
    "train_eval_ratio = evaluate(train_X_max, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X_max, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anneal dataset\n",
    "the info-gain of 31th(index=30) attributes is 0, the info-gain of 12th(index=11) attributes is the max   \n",
    "remove the two cols to see the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the inputdata:  (898, 36)\n",
      "The calsses of the data:  {'2', '1', 'U', '3', '5'}\n",
      "train : val =  718 : 180\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34] :\n",
      "[0.410287 0.       0.300462 0.052846 0.28127  0.144065 0.237197 0.299193\n",
      " 0.111863 0.132557 0.027349 0.456482 0.039277 0.000543 0.040342 0.022473\n",
      " 0.040596 0.03946  0.       0.118099 0.03184  0.028071 0.       0.019241\n",
      " 0.136755 0.       0.02301  0.019264 0.       0.       0.       0.041256\n",
      " 0.02696  0.022722 0.003273]\n",
      "\n",
      "--------------------------    Original        ---------------------------------------------------\n",
      "The evaluation ratio on train set is :  0.8955\n",
      "The evaluation ratio on val set is :  0.8167\n",
      "----------      remove the min info-Gain attributes (0 or almost 0)   ----------------------\n",
      "The evaluation ratio on train set is :  0.8928\n",
      "The evaluation ratio on val set is :  0.8167\n",
      "----------     remove the max info-Gain attributes    ----------------------------------\n",
      "The evaluation ratio on train set is :  0.8426\n",
      "The evaluation ratio on val set is :  0.8111\n"
     ]
    }
   ],
   "source": [
    "file_name = \"anneal\"\n",
    "root = \"2019S1-proj1-data/2019S1-proj1-data\"\n",
    "train_X, train_y, val_X, val_y = preprocess(root + \"/{}.csv\".format(file_name), seed=78)\n",
    "attrs = [i for i in range(train_X.shape[1])]\n",
    "H_cls, H_attrs = info_gain(attrs, train_X, train_y)\n",
    "print(\"Info_gain of {} :\\n{}\".format(attrs,  H_attrs))\n",
    "print()\n",
    "print(\"--------------------------    Original        ---------------------------------------------------\")\n",
    "cls_preds, cond_dict = train(train_X, train_y)\n",
    "train_eval_ratio = evaluate(train_X, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "\n",
    "print(\"----------      remove the min info-Gain attributes (0 or almost 0)   ----------------------\")\n",
    "train_X_min = np.delete(train_X, 30, axis=1)\n",
    "val_X_min = np.delete(val_X, 30, axis=1)\n",
    "cls_preds, cond_dict = train(train_X_min, train_y)\n",
    "train_eval_ratio = evaluate(train_X_min, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X_min, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "\n",
    "print(\"----------     remove the max info-Gain attributes    ----------------------------------\")\n",
    "train_X_max = np.delete(train_X, 11, axis=1)\n",
    "val_X_max = np.delete(val_X, 11, axis=1)\n",
    "cls_preds, cond_dict = train(train_X_max, train_y)\n",
    "train_eval_ratio = evaluate(train_X_max, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X_max, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nursery dataset\n",
    "the info-gain of 3th(index=2) attributes is 0, the info-gain of 9th(index=8) attributes is the max   \n",
    "remove the two cols to see the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the inputdata:  (12960, 9)\n",
      "The calsses of the data:  {'very_recom', 'spec_prior', 'not_recom', 'recommend', 'priority'}\n",
      "train : val =  10368 : 2592\n",
      "Info_gain of [0, 1, 2, 3, 4, 5, 6, 7] :\n",
      "[0.070746 0.197377 0.005211 0.01221  0.019903 0.004543 0.021412 0.95699 ]\n",
      "\n",
      "------------------------      Original         -------------------------------------------\n",
      "The evaluation ratio on train set is :  0.9043\n",
      "The evaluation ratio on val set is :  0.8989\n",
      "----------      remove the min info-Gain attributes  (0 or almost 0)  -----------------------\n",
      "The evaluation ratio on train set is :  0.902\n",
      "The evaluation ratio on val set is :  0.892\n",
      "----------     remove the max info-Gain attributes    --------------------------------------------\n",
      "The evaluation ratio on train set is :  0.5422\n",
      "The evaluation ratio on val set is :  0.5278\n"
     ]
    }
   ],
   "source": [
    "file_name = \"nursery\"\n",
    "root = \"2019S1-proj1-data/2019S1-proj1-data\"\n",
    "train_X, train_y, val_X, val_y = preprocess(root + \"/{}.csv\".format(file_name), seed=78)\n",
    "attrs = [i for i in range(train_X.shape[1])]\n",
    "H_cls, H_attrs = info_gain(attrs, train_X, train_y)\n",
    "print(\"Info_gain of {} :\\n{}\".format(attrs,  H_attrs))\n",
    "print()\n",
    "print(\"------------------------      Original         -------------------------------------------\")\n",
    "cls_preds, cond_dict = train(train_X, train_y)\n",
    "train_eval_ratio = evaluate(train_X, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "\n",
    "print(\"----------      remove the min info-Gain attributes  (0 or almost 0)  -----------------------\")\n",
    "train_X_min = np.delete(train_X, 2, axis=1)\n",
    "val_X_min = np.delete(val_X, 2, axis=1)\n",
    "cls_preds, cond_dict = train(train_X_min, train_y)\n",
    "train_eval_ratio = evaluate(train_X_min, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X_min, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "\n",
    "print(\"----------     remove the max info-Gain attributes    --------------------------------------------\")\n",
    "train_X_max = np.delete(train_X, -1, axis=1)\n",
    "val_X_max = np.delete(val_X, -1, axis=1)\n",
    "cls_preds, cond_dict = train(train_X_max, train_y)\n",
    "train_eval_ratio = evaluate(train_X_max, train_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "eval_ratio = evaluate(val_X_max, val_y, cls_preds, cond_dict)\n",
    "print(\"The evaluation ratio on val set is : \", eval_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses For Question4\n",
    "I have used the Hold-Out method when implementing the naive Bayesian algorithm in the fuction preprocess(data_path, split_ratio=0.2, seed=None).I divide the data set by default 4:, the 4 is the train set and the 1 is the validation set.the result are below.\n",
    "\n",
    "| dataset       |  anneal  | breast-cancer  | car    | cmc |  hepatitis  | hypothyroid | mushroom|nursery|primary-tumor|\n",
    "| ---------------- | :--------: |:--------:   | ----:  |   ----: | :--------: | ----: |   ----: |----: |----: |\n",
    "| count         | 898    | 286     |  1728 |  1473 |  155 |   3163 |  8124 |  12960|  339 |\n",
    "| train         | 0.8983    | 0.7719     |  0.8777 |  0.5102 |  0.8226 |   0.953 |  0.9566 |  0.9045 |  0.5978 |\n",
    "| validation     | 0.8889    | 0.7069    |  0.8699 |  0.4746 | 0.871 |   0.9479 |  0.9625 | 0.9055 | 0.5147 |\n",
    "\n",
    "#### Conclusion\n",
    "- We can find that the more data we have, the smaller the error between the training set and the validation set. May be that the larger the data is , the more likely it is to react to the probability characteristics.\n",
    "- I used a random seed which equals 24 to get the above table.When I use random seed, the result some dataset will cahnge a lot, especificly the dataset with little size data. The more a dataset is, the more steady the correct rate will be.\n",
    "- Normally, the correct rate on the training set is higher than on the validation set.But Sometimes, the correct rate on the training set is a littlr lower than on the validation set. That is intersting.\n",
    "The experiments are above,in the block of \"test on the 9 dataset\"\n",
    "-------------------------------------------------------------\n",
    " I found information about K-fold Cross Validation online and implemented it below.K-CV method can make full use of the data!And I evaluate the car.csv. Its results are more credible and accurate.     \n",
    "- CORRECT RATE ON TRAIN 0.87978     \n",
    "- CORRECT RATE ON VAL 0.8570800000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "import csv\n",
    "import numpy as np\n",
    "def new_preprocess(data_path, split_ratio=0.2, seed=None, start=0):\n",
    "    '''\n",
    "    prpross the data from data_path --- a csv format csv file\n",
    "    @data_path: the path of the csv format file\n",
    "    @split_ratio: the ratio of the val data \n",
    "    '''\n",
    "    #read data from csv\n",
    "    with open(data_path, newline='') as csvfile:\n",
    "        lines = csv.reader(csvfile, delimiter=',')  \n",
    "        matrix = np.array(list(lines))\n",
    "    classes = matrix[:,-1]\n",
    "    \n",
    "    length = matrix.shape[0]\n",
    "    #shuffle the data by using the random seed or if seed == None random else use the seed th shuffle\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    np.random.shuffle(matrix)\n",
    "    matrix = np.vstack((matrix, matrix))\n",
    "    #split the data into train and validation\n",
    "    train, val = matrix[int(length*(start)):int(length*(start + 0.8)),:], matrix[int(length*(start + 0.8)):int(length*(start + 1)),:]\n",
    "    print(\"train : val = \",str(len(train)) + \" : \" + str(len(val)))\n",
    "    \n",
    "    #split the attribues and the classes\n",
    "    train_X, train_y = train[:,:-1], train[:,-1]\n",
    "    val_X, val_y = val[:,:-1], val[:,-1]\n",
    "    return train_X, train_y, val_X, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================     car        ====================================\n",
      "train : val =  1382 : 346\n",
      "The evaluation ratio on train set is :  0.8661\n",
      "The evaluation ratio on val set is :  0.8439\n",
      "train : val =  1383 : 345\n",
      "The evaluation ratio on train set is :  0.8648\n",
      "The evaluation ratio on val set is :  0.8464\n",
      "train : val =  1382 : 346\n",
      "The evaluation ratio on train set is :  0.8763\n",
      "The evaluation ratio on val set is :  0.8497\n",
      "train : val =  1383 : 345\n",
      "The evaluation ratio on train set is :  0.867\n",
      "The evaluation ratio on val set is :  0.8754\n",
      "train : val =  1382 : 346\n",
      "The evaluation ratio on train set is :  0.8726\n",
      "The evaluation ratio on val set is :  0.8699\n",
      "======================        SUMMARY    =========================\n",
      "CORRECT RATE ON TRAIN 0.86936\n",
      "CORRECT RATE ON VAL 0.85706\n"
     ]
    }
   ],
   "source": [
    "file_name = \"car\"\n",
    "root = \"2019S1-proj1-data/2019S1-proj1-data\"   \n",
    "print(\"=======================     {}        ====================================\".format(file_name))\n",
    "train_eval_ratio_list, eval_ratio_list = [], [] \n",
    "for start in [0,0.2,0.4,0.6,0.8]:\n",
    "    train_X, train_y, val_X, val_y = new_preprocess(root + \"/{}.csv\".format(file_name), start=start)\n",
    "    cls_preds, cond_dict = train(train_X, train_y)\n",
    "    train_eval_ratio = evaluate(train_X, train_y, cls_preds, cond_dict)\n",
    "    print(\"The evaluation ratio on train set is : \", train_eval_ratio)\n",
    "    train_eval_ratio_list.append(train_eval_ratio)\n",
    "    eval_ratio = evaluate(val_X, val_y, cls_preds, cond_dict)\n",
    "    print(\"The evaluation ratio on val set is : \", eval_ratio)\n",
    "    eval_ratio_list.append(eval_ratio)\n",
    "print(\"======================        SUMMARY    =========================\")\n",
    "print(\"CORRECT RATE ON TRAIN\", sum(train_eval_ratio_list) / len(train_eval_ratio_list))\n",
    "print(\"CORRECT RATE ON VAL\", sum(eval_ratio_list) / len(eval_ratio_list))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
